# -*- coding: utf-8 -*-
"""

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19TZV3pSSLS1zsOV13tciPFRpbBrbWxgo
"""

#Install required libraries (for Colab environment)
!pip install -U "tensorflow-text==2.15.*"
!pip install -U "tf-models-official==2.15.*"
#-----------------------------------------------------------------
#Imports
import numpy as np
import pandas as pd
import kagglehub
import os
from sklearn.model_selection import train_test_split
import tensorflow_hub as hub
import tensorflow_text as text
from sklearn.metrics.pairwise import cosine_similarity
import tensorflow as tf
from tensorflow.keras import layers,Model,metrics
#----------------------------------------------------------------------

# Download and explore dataset
# -----------------------------
# Download spam dataset from KaggleHub
path = kagglehub.dataset_download("ozlerhakan/spam-or-not-spam-dataset")
print("Path to dataset files:", path)
print("Downloaded to:", path)

# Show all files in the dataset directory
for root, dirs, files in os.walk(path):
    for file in files:
        print(os.path.join(root, file))
        
# Load the dataset
df = pd.read_csv('/kaggle/input/spam-or-not-spam-dataset/spam_or_not_spam.csv')
print('Shape of data:', df.shape)

# Display first few rows
df.head()
#------------------------------------------------------------------

#Analyze and balance the dataset
#--------------------------------------------------------------------
# Check if dataset is imbalanced
print("number of spam and not spam email",df['label'].value_counts())

# Downsample non-spam examples to balance the dataset
df_spam=df[df.label==1]
blnc_num=df_spam.shape[0]
df_not_spam=df[df.label==0]
df_not_spam_blnced=df_not_spam.sample(blnc_num)
df_blnced=pd.concat([df_spam,df_not_spam_blnced])
print('The shape of the balanced dataset:',df_blnced.shape)

#display some rows of the balanced dataset
print(df_blnced.head())
#------------------------------------------------------------------

#Prepare data for training
# -----------------------------------------------------------------
# Define X (emails) and Y (spam or not)
X=df_blnced['email']
Y=df_blnced['label']

# Split into training and testing sets
X_train,X_test,Y_train,Y_test=train_test_split(X,Y)

# Ensure inputs are string type and labels are float type
X_train = pd.Series(X_train).astype(str).to_numpy()
X_test = pd.Series(X_test).astype(str).to_numpy()
Y_train = pd.Series(Y_train).fillna(0).astype(float).to_numpy()
Y_test = pd.Series(Y_test).fillna(0).astype(float).to_numpy()

print('shape of X_train:',X_train.shape)
print('shape of Y_train:',Y_train.shape)
print('shape of X_test:',X_test.shape)
print('shape of Y_tesr:',Y_test.shape)
#---------------------------------------------------------------------

#Load BERT preprocessing and encoder models
# ----------------------------------------------------------------------
# Preprocessing model: tokenization, lowercasing, adding special tokens
bert_preprocessor = hub.KerasLayer(
    "https://kaggle.com/models/tensorflow/bert/TensorFlow2/en-uncased-preprocess/3")

# BERT encoder: generates embeddings
bert_encoder = hub.KerasLayer(
    "https://www.kaggle.com/models/tensorflow/bert/TensorFlow2/en-uncased-l-12-h-768-a-12/4")
#-------------------------------------------------------------------------

#Test BERT output (optional check)
# -------------------------------------------------------------------------
# Encode a few sample sentences
def get_sentence(sentence):
  preprocessed_text=bert_preprocessor(sentence)
  return bert_encoder(preprocessed_text)['pooled_output']

# Example encoding
e=get_sentence(['banana','apple','bill gates'])
print(e)

# # Compute cosine similarity between 'banana' and 'apple'
cosine_similarity([e[0]],[e[1]])
#--------------------------------------------------------------------------------

#Build spam classification model
# -------------------------------------------------------------------------------
# Define model input
text_input = layers.Input(shape=(), dtype=tf.string)

# Preprocess and encode text
encoder_inputs = bert_preprocessor(text_input)
outputs=bert_encoder(encoder_inputs)

# Add dropout for regularization
l=layers.Dropout(0.1,name='dropout')(outputs['pooled_output'])

# Add dense layers
l=layers.Dense(128,activation='relu')(l)
l=layers.Dense(1,activation='sigmoid',name='output')(l)

# Create the model
model = Model(inputs=[text_input], outputs=[l])
#------------------------------------------------------------------------------

#Compile the model
# -----------------------------------------------------------------------------
METRICS=[
    metrics.BinaryAccuracy(name='Accuracy'),
    metrics.Precision(name='precision'),
    metrics.Recall(name='recall')
]
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=METRICS)

## Display model summary
model.summary()
#-------------------------------------------------------------------------------

#Train the model
# ------------------------------------------------------------------------------
model.fit(X_train,Y_train,epochs=15)
#-------------------------------------------------------------------------------

#Evaluate the model
# ------------------------------------------------------------------------------
model.evaluate(X_test,Y_test)
